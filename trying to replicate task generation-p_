{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9910753a-0935-4495-8ab0-35f3253bacd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9fadc6-1e5d-4d1c-b6bd-c7e2c2cb9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data properly with encoding and file handling\n",
    "with open(\"frankenstein-2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deeaee64-f52f-40bf-b403-68a0cac65615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffdf1440-c4d2-4aad-9940-a730d40bef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c86e0ad-b76a-4c04-8629-940b27da3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars to numbers\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = {c: i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "657604e2-9bed-4566-a08c-5b6eb2856adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 659\n",
      "Total vocab: 24\n"
     ]
    }
   ],
   "source": [
    "# Check input and vocabulary size\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2993e83b-9a1f-40b7-8451-3629d1770c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load data\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        processed_inputs = file.readlines()\n",
    "    print(\"Total number of characters:\", len(processed_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e3cbb3d-b3cb-477b-ba45-454687c4628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare data\n",
    "seq_length = 10\n",
    "data = []\n",
    "for i in range(len(processed_inputs) - seq_length):\n",
    "    seq_in = processed_inputs[i:i + seq_length]\n",
    "    data.append(seq_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bd867-bab9-45e8-9a92-743f88fd60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Reshape data into numpy array\n",
    "import numpy as np\n",
    "X = np.reshape(data, (len(data), seq_length, 1))\n",
    "# Total patterns\n",
    "total_patterns = len(data)\n",
    "print(\"Total patterns:\", total_patterns)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10fd250-b7a1-4a9a-a283-37c58f7daa37",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_text_file.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 0: Load your text data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Example: reading from a text file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_text_file.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mlower()  \u001b[38;5;66;03m# lowercase for consistency\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Step 1: Prepare the character list and mappings\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_text_file.txt'"
     ]
    }
   ],
   "source": [
    "# Step 0: Load your text data\n",
    "# Example: reading from a text file\n",
    "with open('your_text_file.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read().lower()  # lowercase for consistency\n",
    "\n",
    "# Step 1: Prepare the character list and mappings\n",
    "chars = sorted(list(set(text)))  # unique characters\n",
    "char_to_num = {char: num for num, char in enumerate(chars)}  # char → num\n",
    "num_to_char = {num: char for char, num in char_to_num.items()}  # num → char\n",
    "\n",
    "# Step 2: Create the processed input\n",
    "processed_inputs = text  # if using characters directly\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)\n",
    "\n",
    "# Step 3: Create sequences\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    \n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "057623ed-cfdf-490b-8c83-02a9ac5f04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a simple example text for testing character-level modeling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a22b273-44bf-4fa0-9393-75eb582fb92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 67\n",
      "Total vocab: 21\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_to_num = {char: num for num, char in enumerate(chars)}\n",
    "num_to_char = {num: char for char, num in char_to_num.items()}\n",
    "\n",
    "processed_inputs = text\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c06471-3acd-433e-a3b5-19d6f915e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 340000\n",
      "Total vocab: 21\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import required libraries\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Load your text (replace with your actual text or file reading)\n",
    "text = \"this is a simple example text for testing character-level modeling. \" * 5000  # dummy long text\n",
    "text = text.lower()\n",
    "\n",
    "# Step 3: Prepare vocabulary\n",
    "chars = sorted(list(set(text)))  # unique characters\n",
    "char_to_num = {char: num for num, char in enumerate(chars)}  # char to index\n",
    "num_to_char = {num: char for char, num in char_to_num.items()}  # index to char\n",
    "\n",
    "# Step 4: Preprocess input\n",
    "processed_inputs = text\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "\n",
    "print(\"Total number of characters:\", input_len)\n",
    "print(\"Total vocab:\", vocab_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f462055d-f927-4b38-a118-3e823ba420f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 339900\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Sequence generation\n",
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "    \n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "n_patterns = len(x_data)\n",
    "print(\"Total Patterns:\", n_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2360c842-e304-40da-9d25-0818ba61d68e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'np_utils' from 'keras.utils' (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\api\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(vocab_len)  \u001b[38;5;66;03m# normalize values to [0, 1]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Optional: convert y_data to one-hot encoding (if using categorical output)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m np_utils\n\u001b[0;32m      7\u001b[0m y \u001b[38;5;241m=\u001b[39m np_utils\u001b[38;5;241m.\u001b[39mto_categorical(y_data)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'np_utils' from 'keras.utils' (C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\api\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Step 6: Convert to numpy array and normalize\n",
    "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X / float(vocab_len)  # normalize values to [0, 1]\n",
    "\n",
    "# Optional: convert y_data to one-hot encoding (if using categorical output)\n",
    "from keras.utils import np_utils\n",
    "y = np_utils.to_categorical(y_data)\n",
    "\n",
    "print(\"Input shape:\", X.shape)\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c3454a-b1d4-4bef-b855-c33c200e1151",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
